{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import sys\n",
    "import math\n",
    "import yaml\n",
    "import nltk\n",
    "import torch\n",
    "import joblib\n",
    "import zipfile\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from textwrap import fill\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installl requirements.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def config_files():\n",
    "    with open(\"../notebook_config.yml\", \"r\") as config_file:\n",
    "        return yaml.safe_load(config_file)\n",
    "\n",
    "\n",
    "def dump_file(value: None, filename: None):\n",
    "    if (value is not None) and (filename is not None):\n",
    "        joblib.dump(value=value, filename=filename)\n",
    "    else:\n",
    "        print(\"Error: 'value' and 'filename' must be provided.\".capitalize())\n",
    "\n",
    "\n",
    "def load_file(filename: None):\n",
    "    if filename is not None:\n",
    "        return joblib.load(filename=filename)\n",
    "    else:\n",
    "        print(\"Error: 'filename' must be provided.\".capitalize())\n",
    "\n",
    "\n",
    "def weight_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        nn.init.kaiming_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"Conv\") != -1:\n",
    "        nn.init.kaiming_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def device_init(device: str = \"cuda\"):\n",
    "    if device == \"cuda\":\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    elif device == \"mps\":\n",
    "        return torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def clean_folders():\n",
    "    TRAIN_MODELS: str = \"../artifacts/checkpoints/train_models/\"\n",
    "    BEST_MODEL: str = \"../artifacts/checkpoints/best_model/\"\n",
    "    METRICS_PATH: str = \"../artifacts/metrics/\"\n",
    "    TRAIN_IMAGES: str = \"../artifacts/outputs/train_images/\"\n",
    "    TEST_IMAGE: str = \"../artifacts/outputs/test_image/\"\n",
    "\n",
    "    warnings.warn(\n",
    "        \"Warning! This will remove the previous files, which might be useful in the future. \"\n",
    "        \"You may want to download the previous files or use MLflow to track and manage them. \"\n",
    "        \"Suggestions for updating them are welcome.\"\n",
    "    )\n",
    "\n",
    "    for path in tqdm(\n",
    "        [TRAIN_MODELS, BEST_MODEL, METRICS_PATH, TRAIN_IMAGES, TEST_IMAGE]\n",
    "    ):\n",
    "        for files in os.listdir(path):\n",
    "            file_path = os.path.join(path, files)\n",
    "            try:\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.remove(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred while deleting {file_path}: {e}\")\n",
    "\n",
    "        print(\"{} folders completed\".format().capitalize())\n",
    "\n",
    "\n",
    "def text_preprocessing(instance):\n",
    "    instance = re.sub(r'[\\n\\'\"()]+|XXXX|x-\\d{4}', \"\", instance)\n",
    "    instance = re.sub(r\"[^a-wy-zA-WY-Z\\s]\", \"\", instance)\n",
    "\n",
    "    instance = instance.lower()\n",
    "\n",
    "    instance = \" \".join(word for word in instance.split() if word not in stop_words)\n",
    "\n",
    "    return instance\n",
    "\n",
    "def plot_images(\n",
    "    predicted: bool = False, device: str = \"cuda\", model=None, epoch: int = 1\n",
    "):\n",
    "    processed_path = config_files()[\"artifacts\"][\"processed_data_path\"]\n",
    "    train_images_path = config_files()[\"artifacts\"][\"train_images\"]\n",
    "    try:\n",
    "        train_dataloader = load_file(\n",
    "            filename=os.path.join(processed_path, \"train_dataloader.pkl\")\n",
    "        )\n",
    "        vocabularies = pd.read_csv(os.path.join(processed_path, \"vocabulary.csv\"))\n",
    "        vocabularies[\"index\"] = vocabularies[\"index\"].astype(int)\n",
    "\n",
    "        images, texts, labels = next(iter(train_dataloader))\n",
    "\n",
    "        predict = model(image=images.to(device), text=texts.to(device))\n",
    "        predict = torch.where(predict > 0.5, 1, 0)\n",
    "        predict = predict.detach().cpu().numpy()\n",
    "\n",
    "        num_images = images.size(0)\n",
    "        num_rows = int(math.sqrt(num_images))\n",
    "        num_cols = math.ceil(num_images / num_rows)\n",
    "\n",
    "        _, axes = plt.subplots(num_rows, num_cols, figsize=(12, 8))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for index, (image, ax) in enumerate(zip(images, axes)):\n",
    "            image = image.squeeze().permute(1, 2, 0).detach().cpu().numpy()\n",
    "            image = (image - image.min()) / (image.max() - image.min())\n",
    "            label = labels[index].item()\n",
    "            text_sequences = texts[index].detach().cpu().numpy().tolist()\n",
    "            words = vocabularies[vocabularies[\"index\"].isin(text_sequences)][\n",
    "                \"vocabulary\"\n",
    "            ].tolist()\n",
    "            medical_report = \" \".join(words).replace(\"<UNK>\", \"\").strip()\n",
    "            wrapped_report = fill(medical_report, width=30)\n",
    "\n",
    "            if predicted:\n",
    "                title_text = f\"**Label**: {label}\\n**Predicted**: {predict[index]}\\nReport: {wrapped_report}\".title()\n",
    "            else:\n",
    "                title_text = f\"Label: {label}\\nReport: {wrapped_report}\"\n",
    "\n",
    "            ax.set_title(title_text, fontsize=9, loc=\"center\")\n",
    "            ax.imshow(image)\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(train_images_path, \"image{}.png\".format(epoch)))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in display_images: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int = 3,\n",
    "        patch_size: int = 16,\n",
    "        image_size: int = 128,\n",
    "        dimension: int = None,\n",
    "    ):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "\n",
    "        self.in_channels = channels\n",
    "        self.patch_size = patch_size\n",
    "        self.dimension = dimension\n",
    "        self.image_size = image_size\n",
    "\n",
    "        if self.dimension is None:\n",
    "            self.dimension = (self.patch_size**2) * self.in_channels\n",
    "\n",
    "        self.number_of_pathches = (self.image_size // self.patch_size) ** 2\n",
    "\n",
    "        self.kernel_size, self.stride_size = self.patch_size, self.patch_size\n",
    "        self.padding_size = self.patch_size // self.patch_size\n",
    "\n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.dimension,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride_size,\n",
    "            padding=self.padding_size,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.positional_embeddings = nn.Parameter(\n",
    "            torch.randn(self.padding_size, self.number_of_pathches, self.dimension),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = self.projection(x)\n",
    "            x = x.view(x.size(0), x.size(1), -1)\n",
    "            x = x.permute(0, 2, 1)\n",
    "            x = self.positional_embeddings + x\n",
    "            return x\n",
    "        else:\n",
    "            raise ValueError(\"Input must be a torch tensor.\".capitalize())\n",
    "\n",
    "    @staticmethod\n",
    "    def total_params(model):\n",
    "        if isinstance(model, PatchEmbedding):\n",
    "            return sum(params.numel() for params in model.parameters())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_channels = config_files()[\"patchEmbeddings\"][\"channels\"]\n",
    "    image_size = config_files()[\"patchEmbeddings\"][\"image_size\"]\n",
    "    patch_size = config_files()[\"patchEmbeddings\"][\"patch_size\"]\n",
    "    dimension = config_files()[\"patchEmbeddings\"][\"dimension\"]\n",
    "\n",
    "    pathEmbedding = PatchEmbedding(\n",
    "        channels=image_channels,\n",
    "        patch_size=patch_size,\n",
    "        image_size=image_size,\n",
    "        dimension=dimension,\n",
    "    )\n",
    "\n",
    "    image = torch.randn(\n",
    "        (image_channels // image_channels, image_channels, image_size, image_size)\n",
    "    )\n",
    "\n",
    "    assert (pathEmbedding(image).size()) == (\n",
    "        image_channels // image_channels,\n",
    "        (image_size // patch_size) ** 2,\n",
    "        dimension,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Product -> self attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(query: torch.Tensor, key: torch.Tensor, values: torch.Tensor):\n",
    "    if (\n",
    "        isinstance(query, torch.Tensor)\n",
    "        and isinstance(key, torch.Tensor)\n",
    "        and isinstance(values, torch.Tensor)\n",
    "    ):\n",
    "        logits = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(query.size(-1))\n",
    "        attention_weights = torch.softmax(logits, dim=-1)\n",
    "        attention_output = torch.matmul(attention_weights, values)\n",
    "        return attention_output\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"All inputs must be torch tensors.\".capitalize())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = torch.randn((1, 8, 64, 32))\n",
    "    key = torch.randn((1, 8, 64, 32))\n",
    "    values = torch.randn((1, 8, 64, 32))\n",
    "\n",
    "    assert (scaled_dot_product(query, key, values).size()) == (1, 8, 64, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention Layer-> attention is all you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, nheads: int = 8, dimension: int = 256):\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "        self.nheads = nheads\n",
    "        self.dimension = dimension\n",
    "\n",
    "        warnings.warn(\n",
    "            \"Please ensure that the dimension is a multiple of the number of heads in the encoder block (e.g., 256 % 8 = 0). \"\n",
    "            \"This is a requirement for the Transformer Encoder Block to function properly. \"\n",
    "            \"If not, you might need to adjust the dimension or the number of heads.\"\n",
    "        )\n",
    "        assert (\n",
    "            dimension % self.nheads == 0\n",
    "        ), \"Dimension mismatched with nheads and dimension\".title()\n",
    "\n",
    "        self.QKV = nn.Linear(\n",
    "            in_features=self.dimension, out_features=3 * self.dimension, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            QKV = self.QKV(x)\n",
    "            query, key, values = torch.chunk(input=QKV, chunks=3, dim=-1)\n",
    "            query = query.view(\n",
    "                query.size(0), query.size(1), self.nheads, self.dimension // self.nheads\n",
    "            )\n",
    "            key = key.view(\n",
    "                key.size(0), key.size(1), self.nheads, self.dimension // self.nheads\n",
    "            )\n",
    "            values = values.view(\n",
    "                values.size(0),\n",
    "                values.size(1),\n",
    "                self.nheads,\n",
    "                self.dimension // self.nheads,\n",
    "            )\n",
    "\n",
    "            query = query.permute(0, 2, 1, 3)\n",
    "            key = key.permute(0, 2, 1, 3)\n",
    "            values = values.permute(0, 2, 1, 3)\n",
    "\n",
    "            attention_output = scaled_dot_product(query=query, key=key, values=values)\n",
    "            attention_output = attention_output.view(\n",
    "                attention_output.size(0),\n",
    "                attention_output.size(2),\n",
    "                attention_output.size(1) * attention_output.size(-1),\n",
    "            )\n",
    "\n",
    "            return attention_output\n",
    "        else:\n",
    "            raise ValueError(\"Input must be a torch tensor.\".capitalize())\n",
    "\n",
    "    @staticmethod\n",
    "    def total_params(model):\n",
    "        if isinstance(model, TransformerEncoderBlock):\n",
    "            return sum(params.numel() for params in model.parameters())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nheads = config_files()[\"transfomerEncoderBlock\"][\"nheads\"]\n",
    "    dimension = config_files()[\"patchEmbeddings\"][\"dimension\"]\n",
    "    image_channels = config_files()[\"patchEmbeddings\"][\"channels\"]\n",
    "    image_size = config_files()[\"patchEmbeddings\"][\"image_size\"]\n",
    "    patch_size = config_files()[\"patchEmbeddings\"][\"patch_size\"]\n",
    "    dimension = config_files()[\"patchEmbeddings\"][\"dimension\"]\n",
    "\n",
    "    multihead_attention = MultiHeadAttentionLayer(\n",
    "        nheads=nheads, dimension=dimension\n",
    "    )\n",
    "\n",
    "    image = torch.randn((1, (image_size // patch_size) ** 2, dimension))\n",
    "\n",
    "    assert (multihead_attention(image).size()) == (\n",
    "        (1, (image_size // patch_size) ** 2, dimension)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int = 256,\n",
    "        out_features: int = 4 * 256,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = \"relu\",\n",
    "    ):\n",
    "        super(FeedForwardNeuralNetwork, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "\n",
    "        if activation == \"relu\":\n",
    "            self.activation_function = nn.ReLU()\n",
    "        elif activation == \"gelu\":\n",
    "            self.activation_function = nn.GELU()\n",
    "        elif activation == \"selu\":\n",
    "            self.activation_function = nn.SELU(inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid activation function. Choose from'relu', 'gelu', or'selu'.\"\n",
    "            )\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        for idx in range(2):\n",
    "            self.layers.append(\n",
    "                nn.Linear(in_features=self.in_features, out_features=self.out_features)\n",
    "            )\n",
    "            self.in_features = self.out_features\n",
    "            self.out_features = in_features\n",
    "\n",
    "            if idx == 0:\n",
    "                self.layers.append(self.activation_function)\n",
    "                self.layers.append(nn.Dropout(self.dropout))\n",
    "\n",
    "        self.network = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return self.network(x)\n",
    "        else:\n",
    "            raise ValueError(\"Input must be a torch.Tensor.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    network = FeedForwardNeuralNetwork(\n",
    "        in_features=256,\n",
    "        out_features=4 * 256,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "    )\n",
    "\n",
    "    print(network(torch.randn((1, 64, 256))).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, dimension: int = 256, layer_norm_eps: float = 1e-5):\n",
    "        super(LayerNormalization, self).__init__()\n",
    "        self.dimension = dimension\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "\n",
    "        self.alpha = nn.Parameter(\n",
    "            data=torch.ones((1, 1, self.dimension)), requires_grad=True\n",
    "        )\n",
    "        self.beta = nn.Parameter(\n",
    "            data=torch.zeros((1, 1, self.dimension)), requires_grad=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            mean = torch.mean(x, dim=-1).unsqueeze(-1)\n",
    "            variance = torch.var(x, dim=-1).unsqueeze(-1)\n",
    "\n",
    "            y = (x - mean) / torch.sqrt(variance + self.layer_norm_eps)\n",
    "\n",
    "            return self.alpha * y + self.beta\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Input must be a torch.Tensor.\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    layer_normalization = LayerNormalization(dimension=256)\n",
    "    assert (layer_normalization(torch.randn((1, 64, 256))).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimension: int = 256,\n",
    "        nheads: int = 8,\n",
    "        dim_feedforward: int = 1024,\n",
    "        dropout: float = 0.1,\n",
    "        layer_norm_eps: float = 1e-5,\n",
    "        activation: str = \"relu\",\n",
    "    ):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.dimension = dimension\n",
    "        self.nheads = nheads\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "\n",
    "        self.multihead_attention = MultiHeadAttentionLayer(\n",
    "            nheads=self.nheads, dimension=self.dimension\n",
    "        )\n",
    "        self.layer_normalization = LayerNormalization(\n",
    "            dimension=self.dimension, layer_norm_eps=self.layer_norm_eps\n",
    "        )\n",
    "        self.feed_forward_network = FeedForwardNeuralNetwork(\n",
    "            in_features=self.dimension,\n",
    "            out_features=4 * self.dim_feedforward,\n",
    "            activation=self.activation,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            residual = x\n",
    "\n",
    "            x = self.multihead_attention(x)\n",
    "            x = self.dropout(x)\n",
    "            x = torch.add(x, residual)\n",
    "            x = self.layer_normalization(x)\n",
    "\n",
    "            residual = x\n",
    "\n",
    "            x = self.feed_forward_network(x)\n",
    "            x = self.dropout(x)\n",
    "            x = torch.add(x, residual)\n",
    "            x = self.layer_normalization(x)\n",
    "\n",
    "            return x\n",
    "        else:\n",
    "            raise ValueError(\"Input must be a torch.Tensor.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def total_params(model):\n",
    "        if isinstance(model, TransformerEncoderBlock):\n",
    "            return sum(params.numel() for params in model.parameters())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nheads = config_files()[\"transfomerEncoderBlock\"][\"nheads\"]\n",
    "    dimension = config_files()[\"patchEmbeddings\"][\"dimension\"]\n",
    "    image_channels = config_files()[\"patchEmbeddings\"][\"channels\"]\n",
    "    image_size = config_files()[\"patchEmbeddings\"][\"image_size\"]\n",
    "    patch_size = config_files()[\"patchEmbeddings\"][\"patch_size\"]\n",
    "    dimension = config_files()[\"patchEmbeddings\"][\"dimension\"]\n",
    "    dropout = config_files()[\"transfomerEncoderBlock\"][\"dropout\"]\n",
    "    activation = config_files()[\"transfomerEncoderBlock\"][\"activation\"]\n",
    "    \n",
    "    transformer_encoder_block = TransformerEncoderBlock(\n",
    "        dimension=dimension,\n",
    "        nheads=nheads,\n",
    "        dim_feedforward=dimension,\n",
    "        dropout=dropout,\n",
    "        activation=activation,\n",
    "        layer_norm_eps=1e-5,\n",
    "    )\n",
    "    \n",
    "    print(transformer_encoder_block(torch.randn((1, (image_size // patch_size) ** 2, dimension))).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder -> attention is all you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimension: int = 256,\n",
    "        nheads: int = 8,\n",
    "        num_encoder_layers: int = 6,\n",
    "        dim_feedforward: int = 1024,\n",
    "        dropout: float = 0.0,\n",
    "        layer_norm_eps: float = 1e-05,\n",
    "        activation: str = \"relu\",\n",
    "    ):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.dimension = dimension\n",
    "        self.nheads = nheads\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.activation = activation\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            *[\n",
    "                TransformerEncoderBlock(\n",
    "                    dimension=self.dimension,\n",
    "                    nheads=self.nheads,\n",
    "                    dim_feedforward=self.dim_feedforward,\n",
    "                    dropout=self.dropout,\n",
    "                    activation=self.activation,\n",
    "                )\n",
    "                for _ in tqdm(range(self.num_encoder_layers))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            for layer in self.layers:\n",
    "                x = layer(x=x)\n",
    "            return x\n",
    "        else:\n",
    "            raise ValueError(\"Input must be a torch.Tensor.\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    transformerEncoder = TransformerEncoder(\n",
    "        dimension=256,\n",
    "        nheads=8,\n",
    "        num_encoder_layers=8,\n",
    "        dim_feedforward=1024,\n",
    "        dropout=0.1,\n",
    "        layer_norm_eps=1e-05,\n",
    "        activation=\"relu\",\n",
    "    )\n",
    "\n",
    "    print(transformerEncoder(torch.randn((1, 64, 256))).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function : BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, loss_name: str = \"BCEWithLogitsLoss\", reduction: str = \"mean\"):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.loss_name = loss_name\n",
    "        self.reduction = reduction\n",
    "\n",
    "        if loss_name == \"BCEWithLogitsLoss\":\n",
    "            self.criterion = nn.BCEWithLogitsLoss(reduction=reduction)\n",
    "        elif loss_name == \"BCELoss\":\n",
    "            self.criterion = nn.BCELoss(reduction=reduction)\n",
    "        elif loss_name == \"CCE\":\n",
    "            self.criterion = nn.CrossEntropyLoss(reduction=reduction)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss function: {loss_name}\")\n",
    "\n",
    "    def forward(self, predicted: torch.Tensor, actual: torch.Tensor):\n",
    "        if isinstance(predicted, torch.Tensor) and isinstance(actual, torch.Tensor):\n",
    "            return self.criterion(predicted, actual)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Both predicted and actual must be torch.Tensor.\".capitalize()\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predicted = torch.tensor(\n",
    "        [2.5, -1.0, 1.5, -2.0, 2.0, 3.0, -1.5, -0.5], dtype=torch.float\n",
    "    )\n",
    "    actual = torch.tensor([1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0], dtype=torch.float)\n",
    "\n",
    "    criterion = LossFunction(loss_name=\"BCEWithLogitsLoss\", reduction=\"mean\")\n",
    "    loss = criterion(predicted, actual)\n",
    "\n",
    "    print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int = 3,\n",
    "        patch_size: int = 16,\n",
    "        image_size: int = 128,\n",
    "        dimension: int = 256,\n",
    "        nheads: int = 8,\n",
    "        num_encoder_layers: int = 6,\n",
    "        dim_feedforward: int = 1024,\n",
    "        dropout: float = 0.0,\n",
    "        layer_norm_eps: float = 1e-05,\n",
    "        activation: str = \"relu\",\n",
    "    ):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "        self.patch_size = patch_size\n",
    "        self.image_size = image_size\n",
    "        self.dimension = dimension\n",
    "        self.nheads = nheads\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.activation = activation\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            channels=channels,\n",
    "            patch_size=patch_size,\n",
    "            image_size=image_size,\n",
    "            dimension=dimension,\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(\n",
    "            dimension=self.dimension,\n",
    "            nheads=self.nheads,\n",
    "            num_encoder_layers=self.num_encoder_layers,\n",
    "            dim_feedforward=self.dim_feedforward,\n",
    "            dropout=self.dropout,\n",
    "            layer_norm_eps=self.layer_norm_eps,\n",
    "            activation=self.activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = self.patch_embedding(x)\n",
    "            x = self.transformer_encoder(x)\n",
    "            return x\n",
    "        else:\n",
    "            raise ValueError(\"Input must be a torch.Tensor.\".capitalize())\n",
    "\n",
    "    @staticmethod\n",
    "    def total_params(model):\n",
    "        if isinstance(model, VisionTransformer):\n",
    "            return sum(params.numel() for params in model.parameters())\n",
    "        else:\n",
    "            raise ValueError(\"Input must be a VisionTransformer model.\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_channels = config_files()[\"patchEmbeddings\"][\"channels\"]\n",
    "    image_size = config_files()[\"patchEmbeddings\"][\"image_size\"]\n",
    "    patch_size = config_files()[\"patchEmbeddings\"][\"patch_size\"]\n",
    "    dimension = config_files()[\"patchEmbeddings\"][\"dimension\"]\n",
    "    nheads = config_files()[\"transfomerEncoderBlock\"][\"nheads\"]\n",
    "    activation = config_files()[\"transfomerEncoderBlock\"][\"activation\"]\n",
    "    dropout = config_files()[\"transfomerEncoderBlock\"][\"dropout\"]\n",
    "    num_encoder_layers = config_files()[\"transfomerEncoderBlock\"][\"num_encoder_layers\"]\n",
    "    dimension_feedforward = config_files()[\"transfomerEncoderBlock\"][\n",
    "        \"dimension_feedforward\"\n",
    "    ]\n",
    "    layer_norm_eps = float(config_files()[\"transfomerEncoderBlock\"][\"layer_norm_eps\"])\n",
    "\n",
    "    image = torch.randn((1, image_channels, image_size, image_size))\n",
    "\n",
    "    vision_transformer = VisionTransformer(\n",
    "        channels=image_channels,\n",
    "        patch_size=patch_size,\n",
    "        image_size=image_size,\n",
    "        dimension=dimension,\n",
    "        nheads=nheads,\n",
    "        activation=activation,\n",
    "        dropout=dropout,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        dim_feedforward=dimension_feedforward,\n",
    "        layer_norm_eps=layer_norm_eps,\n",
    "    )\n",
    "\n",
    "    assert vision_transformer(image).size() == (\n",
    "        1,\n",
    "        (image_size // patch_size) ** 2,\n",
    "        dimension,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimension: int = 256,\n",
    "        nheads: int = 8,\n",
    "        num_encoder_layers: int = 6,\n",
    "        dim_feedforward: int = 1024,\n",
    "        dropout: float = 0.0,\n",
    "        layer_norm_eps: float = 1e-05,\n",
    "        activation: str = \"relu\",\n",
    "    ):\n",
    "        super(TextTransformerEncoder, self).__init__()\n",
    "        self.dimension = dimension\n",
    "        self.nheads = nheads\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.activation = activation\n",
    "\n",
    "        try:\n",
    "            self.image_size = config_files()[\"patchEmbeddings\"][\"image_size\"]\n",
    "            self.patch_size = config_files()[\"patchEmbeddings\"][\"patch_size\"]\n",
    "            self.image_channels = config_files()[\"patchEmbeddings\"][\"channels\"]\n",
    "        except KeyError:\n",
    "            raise ValueError(\n",
    "                \"Image configuration not found in the config files.\".capitalize()\n",
    "            )\n",
    "        else:\n",
    "            self.sequence_length = (self.image_size // self.patch_size) ** 2\n",
    "\n",
    "        if self.dimension is None:\n",
    "            self.dimension = (self.patch_size**2) * self.image_size\n",
    "\n",
    "        warnings.warn(\n",
    "            \"The number of vocabularies (unique words) is calculated by multiplying the 'dimension with the patch size'. \"\n",
    "            \"If you need a larger vocabulary size, please increase the patch size and image size.\"\n",
    "        )\n",
    "\n",
    "        self.number_of_vocabularies = self.image_size * self.dimension\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.number_of_vocabularies, embedding_dim=self.dimension\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(\n",
    "            dimension=self.dimension,\n",
    "            nheads=self.nheads,\n",
    "            num_encoder_layers=self.num_encoder_layers,\n",
    "            dim_feedforward=self.dim_feedforward,\n",
    "            dropout=self.dropout,\n",
    "            layer_norm_eps=self.layer_norm_eps,\n",
    "            activation=self.activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = self.embedding(x)\n",
    "            x = self.transformer_encoder(x)\n",
    "            return x\n",
    "        else:\n",
    "            raise ValueError(\"Input must be a torch.Tensor.\".capitalize())\n",
    "\n",
    "    @staticmethod\n",
    "    def total_params(model):\n",
    "        if isinstance(model, TextTransformerEncoder):\n",
    "            return sum(params.numel() for params in model.parameters())\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Input must be a TextTransformerEncoder model.\".capitalize()\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_channels = config_files()[\"patchEmbeddings\"][\"channels\"]\n",
    "    image_size = config_files()[\"patchEmbeddings\"][\"image_size\"]\n",
    "    patch_size = config_files()[\"patchEmbeddings\"][\"patch_size\"]\n",
    "    dimension = config_files()[\"patchEmbeddings\"][\"dimension\"]\n",
    "    nheads = config_files()[\"transfomerEncoderBlock\"][\"nheads\"]\n",
    "    activation = config_files()[\"transfomerEncoderBlock\"][\"activation\"]\n",
    "    dropout = config_files()[\"transfomerEncoderBlock\"][\"dropout\"]\n",
    "    num_encoder_layers = config_files()[\"transfomerEncoderBlock\"][\"num_encoder_layers\"]\n",
    "    dimension_feedforward = config_files()[\"transfomerEncoderBlock\"][\n",
    "        \"dimension_feedforward\"\n",
    "    ]\n",
    "    layer_norm_eps = float(config_files()[\"transfomerEncoderBlock\"][\"layer_norm_eps\"])\n",
    "\n",
    "    sequence_length = (image_size // patch_size) ** 2\n",
    "\n",
    "    textual_data = torch.randint(0, sequence_length, (1, sequence_length))\n",
    "\n",
    "    text_transfomer = TextTransformerEncoder(\n",
    "        dimension=dimension,\n",
    "        nheads=nheads,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        dim_feedforward=dimension_feedforward,\n",
    "        dropout=dropout,\n",
    "        layer_norm_eps=layer_norm_eps,\n",
    "        activation=activation,\n",
    "    )\n",
    "\n",
    "    assert (text_transfomer(textual_data).size()) == (1, sequence_length, dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Modal Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, dimension: int = 256):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.in_features = dimension * 2\n",
    "        self.out_features = dimension // 2\n",
    "\n",
    "        self.layers = list()\n",
    "\n",
    "        for idx in range(3):\n",
    "            if idx != 2:\n",
    "                self.layers.append(\n",
    "                    nn.Linear(\n",
    "                        in_features=self.in_features, out_features=self.out_features\n",
    "                    )\n",
    "                )\n",
    "                self.layers.append(nn.ReLU(inplace=True))\n",
    "                self.layers.append(nn.BatchNorm1d(num_features=self.out_features))\n",
    "\n",
    "                self.in_features = self.out_features\n",
    "                self.out_features = self.in_features // 2\n",
    "            else:\n",
    "                self.layers.append(\n",
    "                    nn.Linear(\n",
    "                        in_features=self.in_features,\n",
    "                        out_features=self.in_features // self.in_features,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        self.classifier = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return self.classifier(x).view(-1)\n",
    "        else:\n",
    "            raise ValueError(\"Input must be a torch.Tensor.\".capitalize())\n",
    "\n",
    "    @staticmethod\n",
    "    def total_params(model):\n",
    "        if isinstance(model, Classifier):\n",
    "            return sum(params.numel() for params in model.parameters())\n",
    "        else:\n",
    "            raise ValueError(\"Input must be a Classifier model.\".capitalize())\n",
    "        \n",
    "\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int = 3,\n",
    "        patch_size: int = 16,\n",
    "        image_size: int = 128,\n",
    "        dimension: int = 256,\n",
    "        nheads: int = 8,\n",
    "        num_encoder_layers: int = 6,\n",
    "        dim_feedforward: int = 1024,\n",
    "        dropout: float = 0.1,\n",
    "        layer_norm_eps: float = 1e-05,\n",
    "        activation: str = \"relu\",\n",
    "    ):\n",
    "        super(MultiModalClassifier, self).__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "        self.patch_size = patch_size\n",
    "        self.image_size = image_size\n",
    "        self.dimension = dimension\n",
    "        self.nheads = nheads\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.activation = activation\n",
    "\n",
    "        self.vision_transformer = VisionTransformer(\n",
    "            channels=channels,\n",
    "            patch_size=patch_size,\n",
    "            image_size=image_size,\n",
    "            dimension=dimension,\n",
    "            nheads=nheads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            layer_norm_eps=layer_norm_eps,\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "        self.text_transformer = TextTransformerEncoder(\n",
    "            dimension=self.dimension,\n",
    "            nheads=self.nheads,\n",
    "            num_encoder_layers=self.num_encoder_layers,\n",
    "            dim_feedforward=self.dim_feedforward,\n",
    "            dropout=self.dropout,\n",
    "            layer_norm_eps=self.layer_norm_eps,\n",
    "            activation=self.activation,\n",
    "        )\n",
    "\n",
    "        self.classifier = Classifier(dimension=self.dimension)\n",
    "\n",
    "    def forward(self, image: torch.Tensor, text: torch.Tensor):\n",
    "        if isinstance(image, torch.Tensor) and isinstance(text, torch.Tensor):\n",
    "            image_features = self.vision_transformer(image)\n",
    "            text_features = self.text_transformer(text)\n",
    "\n",
    "            image_features = torch.mean(input=image_features, dim=1)\n",
    "            text_features = torch.mean(input=text_features, dim=1)\n",
    "            fusion = torch.cat((image_features, text_features), dim=1)\n",
    "            classifier = self.classifier(fusion)\n",
    "\n",
    "            return classifier\n",
    "        else:\n",
    "            raise ValueError(\"Both inputs must be torch.Tensor.\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_channels = config_files()[\"patchEmbeddings\"][\"channels\"]\n",
    "    image_size = config_files()[\"patchEmbeddings\"][\"image_size\"]\n",
    "    patch_size = config_files()[\"patchEmbeddings\"][\"patch_size\"]\n",
    "    dimension = config_files()[\"patchEmbeddings\"][\"dimension\"]\n",
    "    nheads = config_files()[\"transfomerEncoderBlock\"][\"nheads\"]\n",
    "    activation = config_files()[\"transfomerEncoderBlock\"][\"activation\"]\n",
    "    dropout = config_files()[\"transfomerEncoderBlock\"][\"dropout\"]\n",
    "    num_encoder_layers = config_files()[\"transfomerEncoderBlock\"][\"num_encoder_layers\"]\n",
    "    dimension_feedforward = config_files()[\"transfomerEncoderBlock\"][\n",
    "        \"dimension_feedforward\"\n",
    "    ]\n",
    "    layer_norm_eps = float(config_files()[\"transfomerEncoderBlock\"][\"layer_norm_eps\"])\n",
    "    batch_size = config_files()[\"dataloader\"][\"batch_size\"]\n",
    "\n",
    "    number_of_patches = (image_size // patch_size) ** 2\n",
    "    number_of_sequences = (image_size // patch_size) ** 2\n",
    "\n",
    "    images = torch.randn(1, image_channels, image_size, image_size)\n",
    "    texts = torch.randint(0, number_of_sequences, (1, number_of_sequences))\n",
    "\n",
    "    classifier = MultiModalClassifier(\n",
    "        channels=image_channels,\n",
    "        patch_size=patch_size,\n",
    "        image_size=image_size,\n",
    "        dimension=dimension,\n",
    "        nheads=nheads,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        dim_feedforward=dimension_feedforward,\n",
    "        dropout=dropout,\n",
    "        layer_norm_eps=layer_norm_eps,\n",
    "        activation=activation,\n",
    "    )\n",
    "    output = classifier(image=images, text=texts)\n",
    "    output = classifier(image=images, text=texts)\n",
    "    assert output.unsqueeze(-1).size() == (\n",
    "        batch_size,\n",
    "        batch_size // batch_size,\n",
    "    ), \"Multi Modal Classifier output size mismatch\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int = 3,\n",
    "        image_size: int = 128,\n",
    "        patch_size: int = 16,\n",
    "        batch_size: int = 4,\n",
    "        split_size: float = 0.25,\n",
    "    ):\n",
    "        self.channels = channels\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.batch_size = batch_size\n",
    "        self.split_size = split_size\n",
    "\n",
    "        self.vocabulary = {\"<UNK>\": 0}\n",
    "        self.images_data = list()\n",
    "        self.labels_data = list()\n",
    "        self.textual_data = list()\n",
    "        self.text_to_sequence = list()\n",
    "        self.sequences = list()\n",
    "\n",
    "        self.sequence_length = (self.image_size // self.patch_size) ** 2\n",
    "\n",
    "    def image_transform(self):\n",
    "        return transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((self.image_size, self.image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.CenterCrop((self.image_size, self.image_size)),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def preprocess_csv_file(self):\n",
    "        dataframe_path = os.path.join(\n",
    "            config_files()[\"artifacts\"][\"raw_data_path\"], \"image_labels_reports.csv\"\n",
    "        )\n",
    "        df = pd.read_csv(dataframe_path)\n",
    "        if (\"text\" in df.columns) and (\"label\" in df.columns) and (\"img\" in df.columns):\n",
    "            labels = df.loc[:, \"label\"]\n",
    "            reports = df.loc[:, \"text\"]\n",
    "            images = df.loc[:, \"img\"]\n",
    "\n",
    "            df[\"text\"] = df[\"text\"].apply(text_preprocessing)\n",
    "            reports = reports.apply(text_preprocessing)\n",
    "\n",
    "            return {\n",
    "                \"labels\": labels,\n",
    "                \"reports\": reports,\n",
    "                \"images\": images,\n",
    "                \"dataframe\": df,\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"The 'text' and 'labels' columns are missing in the CSV file.\".capitalize()\n",
    "            )\n",
    "\n",
    "    def create_vocabularies(self, instance):\n",
    "        for word in instance.split(\" \"):\n",
    "            if word not in self.vocabulary:\n",
    "                self.vocabulary[word] = len(self.vocabulary)\n",
    "\n",
    "    def unzip_image_dataset(self):\n",
    "        if os.path.exists(config_files()[\"artifacts\"][\"raw_data_path\"]):\n",
    "            image_data_path = os.path.join(\n",
    "                config_files()[\"artifacts\"][\"raw_data_path\"], \"image_dataset.zip\"\n",
    "            )\n",
    "            processed_data_path = config_files()[\"artifacts\"][\"processed_data_path\"]\n",
    "\n",
    "            with zipfile.ZipFile(file=image_data_path, mode=\"r\") as zip_file:\n",
    "                zip_file.extractall(\n",
    "                    path=os.path.join(processed_data_path, \"image_dataset\")\n",
    "                )\n",
    "\n",
    "            print(\n",
    "                \"Image dataset unzipped successfully in the folder {}\".capitalize().format(\n",
    "                    processed_data_path\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Could not extract image dataset\".capitalize())\n",
    "\n",
    "    def create_sequences(self, instance, sequence_length: int):\n",
    "        sequence = [\n",
    "            self.vocabulary.get(word, self.vocabulary[\"<UNK>\"])\n",
    "            for word in instance.split()\n",
    "        ]\n",
    "        if len(sequence) > sequence_length:\n",
    "            sequence = sequence[:sequence_length]\n",
    "\n",
    "        elif len(sequence) < sequence_length:\n",
    "            sequence.extend(\n",
    "                [self.vocabulary[\"<UNK>\"]] * (sequence_length - len(sequence))\n",
    "            )\n",
    "\n",
    "        assert (\n",
    "            len(sequence) == sequence_length\n",
    "        ), f\"Error: Sequence length is {len(sequence)} instead of {sequence_length}\"\n",
    "\n",
    "        self.sequences.append(sequence)\n",
    "\n",
    "        return sequence\n",
    "\n",
    "    def extracted_image_and_text_features(self):\n",
    "        dataset = self.preprocess_csv_file()\n",
    "        images = dataset[\"images\"]\n",
    "        labels = dataset[\"labels\"]\n",
    "        reports = dataset[\"reports\"]\n",
    "        dataframe = dataset[\"dataframe\"]\n",
    "\n",
    "        try:\n",
    "            reports.apply(self.create_vocabularies)\n",
    "\n",
    "            pd.DataFrame(\n",
    "                list(self.vocabulary.items()), columns=[\"vocabulary\", \"index\"]\n",
    "            ).to_csv(\n",
    "                os.path.join(\n",
    "                    config_files()[\"artifacts\"][\"processed_data_path\"], \"vocabulary.csv\"\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while creating vocabularies: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        dataframe[\"sequences\"] = reports.apply(\n",
    "            self.create_sequences, sequence_length=self.sequence_length\n",
    "        )\n",
    "\n",
    "        all_image_path = os.path.join(\n",
    "            config_files()[\"artifacts\"][\"processed_data_path\"], \"image_dataset\"\n",
    "        )\n",
    "\n",
    "        for image in tqdm(os.listdir(all_image_path), desc=\"Processing Images\"):\n",
    "            try:\n",
    "                if image not in images.values.tolist():\n",
    "                    print(f\"Image not found in dataset: {image}\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    text = dataframe.loc[dataframe[\"img\"] == image, \"text\"].values[0]\n",
    "                    label = dataframe.loc[dataframe[\"img\"] == image, \"label\"].values[0]\n",
    "                    sequences = dataframe.loc[\n",
    "                        dataframe[\"img\"] == image, \"sequences\"\n",
    "                    ].values[0]\n",
    "                except IndexError:\n",
    "                    print(f\"Missing data for image: {image}\")\n",
    "                    continue\n",
    "\n",
    "                single_image_path = os.path.join(all_image_path, image)\n",
    "\n",
    "                if not single_image_path.lower().endswith((\"jpeg\", \"png\", \"jpg\")):\n",
    "                    print(f\"Invalid file format: {single_image_path}\")\n",
    "                    continue\n",
    "\n",
    "                if not os.path.exists(single_image_path):\n",
    "                    print(f\"File does not exist: {single_image_path}\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    image_data = cv2.imread(single_image_path)\n",
    "                    image_data = cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)\n",
    "                    if image_data is None:\n",
    "                        raise ValueError(\n",
    "                            f\"Corrupted or unreadable image: {single_image_path}\"\n",
    "                        )\n",
    "\n",
    "                    image_data = Image.fromarray(image_data)\n",
    "                    image_data = self.image_transform()(image_data)\n",
    "\n",
    "                    if not isinstance(image_data, torch.Tensor):\n",
    "                        raise TypeError(\n",
    "                            f\"Expected torch.Tensor but got {type(image_data)} for {image}\"\n",
    "                        )\n",
    "\n",
    "                    self.images_data.append(image_data)\n",
    "                    self.labels_data.append(label)\n",
    "                    self.textual_data.append(text)\n",
    "                    self.text_to_sequence.append(sequences)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image '{image}': {e}\")\n",
    "                    continue\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error processing file '{image}': {e}\")\n",
    "\n",
    "        assert (\n",
    "            len(self.images_data)\n",
    "            == len(self.labels_data)\n",
    "            == len(self.textual_data)\n",
    "            == len(self.text_to_sequence)\n",
    "        ), \"Mismatch: 'Image data', 'labels', 'text', and 'text to sequence' are not equal\"\n",
    "\n",
    "        try:\n",
    "            self.labels_data = torch.tensor(self.labels_data, dtype=torch.long)\n",
    "            self.text_to_sequence = torch.tensor(\n",
    "                self.text_to_sequence, dtype=torch.long\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"Tensor conversion failed: {e}\")\n",
    "\n",
    "        return {\n",
    "            \"images\": self.images_data,\n",
    "            \"labels\": self.labels_data,\n",
    "            \"text_to_sequence\": self.text_to_sequence,\n",
    "        }\n",
    "\n",
    "    def create_dataloader(self):\n",
    "        try:\n",
    "            dataset = self.extracted_image_and_text_features()\n",
    "            images = dataset[\"images\"]\n",
    "            labels = dataset[\"labels\"]\n",
    "            text_to_sequence = dataset[\"text_to_sequence\"]\n",
    "\n",
    "            test_image_portion = int(len(images) * self.split_size)\n",
    "            test_labels_portion = int(len(labels) * self.split_size)\n",
    "            test_texts_portion = int(text_to_sequence.size(0) * self.split_size)\n",
    "\n",
    "            train_images = images[:-test_image_portion]\n",
    "            train_labels = labels[:-test_labels_portion]\n",
    "            train_texts = text_to_sequence[:-test_texts_portion]\n",
    "\n",
    "            test_images = images[-test_image_portion:]\n",
    "            test_labels = labels[-test_labels_portion:]\n",
    "            test_texts = text_to_sequence[-test_texts_portion:]\n",
    "\n",
    "            if (\n",
    "                len(train_images) == 0\n",
    "                or len(train_labels) == 0\n",
    "                or len(train_texts) == 0\n",
    "            ):\n",
    "                raise ValueError(\"Train dataset is empty! Check split size.\")\n",
    "            if len(test_images) == 0 or len(test_labels) == 0 or len(test_texts) == 0:\n",
    "                raise ValueError(\"Test dataset is empty! Check split size.\")\n",
    "\n",
    "            train_dataloader = DataLoader(\n",
    "                dataset=list(zip(train_images, train_texts, train_labels)),\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=True,\n",
    "            )\n",
    "            test_dataloader = DataLoader(\n",
    "                dataset=list(zip(test_images, test_texts, test_labels)),\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False,\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                for filename, value in [\n",
    "                    (\"train_dataloader.pkl\", train_dataloader),\n",
    "                    (\"test_dataloader.pkl\", test_dataloader),\n",
    "                ]:\n",
    "                    dump_file(\n",
    "                        value=value,\n",
    "                        filename=os.path.join(\n",
    "                            config_files()[\"artifacts\"][\"processed_data_path\"],\n",
    "                            filename,\n",
    "                        ),\n",
    "                    )\n",
    "\n",
    "                print(\n",
    "                    \"Dataloaders created successfully in the folder {}\".capitalize().format(\n",
    "                        config_files()[\"artifacts\"][\"processed_data_path\"]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            except StopIteration:\n",
    "                raise RuntimeError(\n",
    "                    \"Train dataloader is empty. Check data loading logic.\"\n",
    "                )\n",
    "\n",
    "            return train_dataloader, test_dataloader\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in create_dataloader: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    @staticmethod\n",
    "    def details_dataset():\n",
    "        train_dataloader = load_file(\n",
    "            filename=os.path.join(\n",
    "                config_files()[\"artifacts\"][\"processed_data_path\"],\n",
    "                \"train_dataloader.pkl\",\n",
    "            )\n",
    "        )\n",
    "        test_dataloader = load_file(\n",
    "            filename=os.path.join(\n",
    "                config_files()[\"artifacts\"][\"processed_data_path\"],\n",
    "                \"test_dataloader.pkl\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        train_images, _, train_labels = next(iter(train_dataloader))\n",
    "        _, test_sequences, _ = next(iter(test_dataloader))\n",
    "\n",
    "        total_train_dataset = sum(image.size(0) for image, _, _ in train_dataloader)\n",
    "        total_test_dataset = sum(image.size(0) for image, _, _ in test_dataloader)\n",
    "\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"Dataset\": [\"Train\", \"Test\"],\n",
    "                \"Size\": [total_train_dataset, total_test_dataset],\n",
    "                \"Number of Batches\": [len(train_dataloader), len(test_dataloader)],\n",
    "                \"Image Size\": str([train_images.size()]),\n",
    "                \"Sequence Size\": str([test_sequences.size()]),\n",
    "                \"Label Size\": str([train_labels.size()]),\n",
    "                \"Label Type\": str([train_labels.dtype]),\n",
    "                \"Text Type\": str([test_sequences.dtype]),\n",
    "            }\n",
    "        ).to_csv(\n",
    "            os.path.join(\n",
    "                config_files()[\"artifacts\"][\"processed_data_path\"],\n",
    "                \"dataset_details.csv\",\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            \"Dataset details saved successfully in the folder {}\".capitalize().format(\n",
    "                config_files()[\"artifacts\"][\"processed_data_path\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def display_images():\n",
    "        try:\n",
    "            train_dataloader = load_file(\n",
    "                filename=os.path.join(\n",
    "                    config_files()[\"artifacts\"][\"processed_data_path\"],\n",
    "                    \"train_dataloader.pkl\",\n",
    "                )\n",
    "            )\n",
    "            vocabularies = pd.read_csv(\n",
    "                os.path.join(\n",
    "                    config_files()[\"artifacts\"][\"processed_data_path\"], \"vocabulary.csv\"\n",
    "                )\n",
    "            )\n",
    "            vocabularies[\"index\"] = vocabularies[\"index\"].astype(int)\n",
    "\n",
    "            images, texts, labels = next(iter(train_dataloader))\n",
    "\n",
    "            num_images = images.size(0)\n",
    "            num_rows = int(math.sqrt(num_images))\n",
    "            num_cols = math.ceil(num_images / num_rows)\n",
    "\n",
    "            fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 8))\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            for index, (image, ax) in enumerate(zip(images, axes)):\n",
    "                image = image.squeeze().permute(1, 2, 0).detach().cpu().numpy()\n",
    "                image = (image - image.min()) / (image.max() - image.min())\n",
    "                label = labels[index].item()\n",
    "                text_sequences = texts[index].detach().cpu().numpy().tolist()\n",
    "                words = vocabularies[vocabularies[\"index\"].isin(text_sequences)][\n",
    "                    \"vocabulary\"\n",
    "                ].tolist()\n",
    "                medical_report = \" \".join(words).replace(\"<UNK>\", \"\").strip()\n",
    "                wrapped_report = fill(medical_report, width=30)\n",
    "\n",
    "                title_text = f\"Label: {label}\\nReport: {wrapped_report}\"\n",
    "\n",
    "                ax.set_title(title_text, fontsize=9, loc=\"center\")\n",
    "                ax.imshow(image)\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in display_images: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loader = Loader(\n",
    "        channels = 3,\n",
    "        image_size = 224,\n",
    "        batch_size=4,\n",
    "        split_size=0.25,\n",
    "    )\n",
    "\n",
    "    loader.unzip_image_dataset()\n",
    "    loader.create_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataloader():\n",
    "    processed_data_path = config_files()[\"artifacts\"][\"processed_data_path\"]\n",
    "    if os.path.exists(processed_data_path):\n",
    "        train_dataloader_path = os.path.join(\n",
    "            processed_data_path, \"train_dataloader.pkl\"\n",
    "        )\n",
    "        valid_dataloader_path = os.path.join(processed_data_path, \"test_dataloader.pkl\")\n",
    "\n",
    "        train_dataloader = load_file(filename=train_dataloader_path)\n",
    "        valid_dataloader = load_file(filename=valid_dataloader_path)\n",
    "\n",
    "        return {\n",
    "            \"train_dataloader\": train_dataloader,\n",
    "            \"valid_dataloader\": valid_dataloader,\n",
    "        }\n",
    "\n",
    "\n",
    "def helper(**kwargs):\n",
    "    model = kwargs[\"model\"]\n",
    "    lr: float = kwargs[\"lr\"]\n",
    "    beta1: float = kwargs[\"beta1\"]\n",
    "    beta2: float = kwargs[\"beta2\"]\n",
    "    momentum: float = kwargs[\"momentum\"]\n",
    "    weight_decay: float = kwargs[\"weight_decay\"]\n",
    "    adam: bool = kwargs[\"adam\"]\n",
    "    SGD: bool = kwargs[\"SGD\"]\n",
    "\n",
    "    if model is None:\n",
    "        nheads = config_files()[\"transfomerEncoderBlock\"][\"nheads\"]\n",
    "        dropout = config_files()[\"transfomerEncoderBlock\"][\"dropout\"]\n",
    "        image_size = config_files()[\"patchEmbeddings\"][\"image_size\"]\n",
    "        patch_size = config_files()[\"patchEmbeddings\"][\"patch_size\"]\n",
    "        activation = config_files()[\"transfomerEncoderBlock\"][\"activation\"]\n",
    "        dimension = config_files()[\"patchEmbeddings\"][\"dimension\"]\n",
    "        image_channels = config_files()[\"patchEmbeddings\"][\"channels\"]\n",
    "        num_encoder_layers = config_files()[\"transfomerEncoderBlock\"][\n",
    "            \"num_encoder_layers\"\n",
    "        ]\n",
    "        dimension_feedforward = config_files()[\"transfomerEncoderBlock\"][\n",
    "            \"dimension_feedforward\"\n",
    "        ]\n",
    "        layer_norm_eps = float(\n",
    "            config_files()[\"transfomerEncoderBlock\"][\"layer_norm_eps\"]\n",
    "        )\n",
    "\n",
    "        classifier = MultiModalClassifier(\n",
    "            channels=image_channels,\n",
    "            patch_size=patch_size,\n",
    "            image_size=image_size,\n",
    "            nheads=nheads,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            dimension=dimension,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            dim_feedforward=dimension_feedforward,\n",
    "            layer_norm_eps=layer_norm_eps,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        classifier = model\n",
    "\n",
    "    if adam:\n",
    "        optimizer = optim.Adam(\n",
    "            params=classifier.parameters(),\n",
    "            lr=lr,\n",
    "            betas=(beta1, beta2),\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "    elif SGD:\n",
    "        optimizer = optim.SGD(params=classifier.parameters(), lr=lr, momentum=momentum)\n",
    "    else:\n",
    "        raise ValueError(\"Optimizer not supported\".capitalize())\n",
    "\n",
    "    criterion = LossFunction(loss_name=\"BCEWithLogitsLoss\", reduction=\"mean\")\n",
    "\n",
    "    try:\n",
    "        dataset = load_dataloader()\n",
    "    except Exception as e:\n",
    "        print(f\"Error while loading dataset: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    return {\n",
    "        \"train_dataloader\": dataset[\"train_dataloader\"],\n",
    "        \"test_dataloader\": dataset[\"valid_dataloader\"],\n",
    "        \"model\": classifier,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"criterion\": criterion,\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    init = helper(\n",
    "        model = None,\n",
    "        lr = float(config_files()[\"trainer\"][\"lr\"]),\n",
    "        beta1 = config_files()[\"trainer\"][\"beta1\"],\n",
    "        beta2 = config_files()[\"trainer\"][\"beta2\"],\n",
    "        momentum = config_files()[\"trainer\"][\"momentum\"],\n",
    "        weight_decay = float(config_files()[\"trainer\"][\"weight_decay\"]),\n",
    "        adam = config_files()[\"trainer\"][\"adam\"],\n",
    "        SGD = config_files()[\"trainer\"][\"SGD\"],\n",
    "    )\n",
    "\n",
    "    assert init[\"model\"].__class__ == MultiModalClassifier\n",
    "    assert init[\"optimizer\"].__class__ == optim.Adam\n",
    "    assert init[\"criterion\"].__class__ == LossFunction\n",
    "    assert init[\"train_dataloader\"].__class__ == torch.utils.data.DataLoader\n",
    "    assert init[\"test_dataloader\"].__class__ == torch.utils.data.DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=None,\n",
    "        epochs: int = 100,\n",
    "        lr: float = 2e-4,\n",
    "        beta1: float = 0.5,\n",
    "        beta2: float = 0.999,\n",
    "        momentum: float = 0.95,\n",
    "        step_size: int = 20,\n",
    "        gamma: float = 0.75,\n",
    "        l1_lambda: float = 0.01,\n",
    "        l2_lambda: float = 0.01,\n",
    "        device: str = \"cuda\",\n",
    "        adam: bool = True,\n",
    "        SGD: bool = False,\n",
    "        l1_regularization: bool = False,\n",
    "        l2_regularization: bool = False,\n",
    "        lr_scheduler: bool = False,\n",
    "        verbose: bool = True,\n",
    "        mlflow: bool = False,\n",
    "    ):\n",
    "        try:\n",
    "            self.model = model\n",
    "            self.epochs = epochs\n",
    "            self.lr = lr\n",
    "            self.beta1 = beta1\n",
    "            self.beta2 = beta2\n",
    "            self.momentum = momentum\n",
    "            self.step_size = step_size\n",
    "            self.gamma = gamma\n",
    "            self.l1_lambda = l1_lambda\n",
    "            self.l2_lambda = l2_lambda\n",
    "            self.adam = adam\n",
    "            self.SGD = SGD\n",
    "            self.device = device\n",
    "            self.l1_regularization = l1_regularization\n",
    "            self.l2_regularization = l2_regularization\n",
    "            self.lr_scheduler = lr_scheduler\n",
    "            self.verbose = verbose\n",
    "            self.mlflow = mlflow\n",
    "\n",
    "            self.device = device_init(device=self.device)\n",
    "\n",
    "            self.init = helper(\n",
    "                model=self.model,\n",
    "                lr=self.lr,\n",
    "                beta1=self.beta1,\n",
    "                beta2=self.beta2,\n",
    "                momentum=self.momentum,\n",
    "                weight_decay=self.l2_lambda,\n",
    "                adam=self.adam,\n",
    "                SGD=self.SGD,\n",
    "            )\n",
    "            try:\n",
    "                self.train_dataloader = self.init[\"train_dataloader\"]\n",
    "                self.test_dataloader = self.init[\"test_dataloader\"]\n",
    "                assert (\n",
    "                    self.train_dataloader.__class__ == torch.utils.data.DataLoader\n",
    "                ), \"Train_dataloader is not a valid DataLoader\"\n",
    "                assert (\n",
    "                    self.test_dataloader.__class__ == torch.utils.data.DataLoader\n",
    "                ), \"Test_dataloader is not a valid DataLoader\"\n",
    "            except KeyError as e:\n",
    "                print(f\"DataLoader Initialization Error: Missing key {e}\")\n",
    "                sys.exit(1)\n",
    "\n",
    "            try:\n",
    "                self.model = self.init[\"model\"]\n",
    "                assert (\n",
    "                    self.model.__class__ == MultiModalClassifier\n",
    "                ), \"Model must be an instance of MultiModalClassifier\"\n",
    "            except KeyError:\n",
    "                print(\n",
    "                    \"Model Initialization Error: 'model' key missing from helper return dictionary\"\n",
    "                )\n",
    "                sys.exit(1)\n",
    "            except AssertionError as e:\n",
    "                print(e)\n",
    "                sys.exit(1)\n",
    "\n",
    "            try:\n",
    "                self.optimizer = self.init[\"optimizer\"]\n",
    "                if self.adam:\n",
    "                    assert (\n",
    "                        self.optimizer.__class__ == optim.Adam\n",
    "                    ), \"Optimizer should be Adam\"\n",
    "                elif self.SGD:\n",
    "                    assert (\n",
    "                        self.optimizer.__class__ == optim.SGD\n",
    "                    ), \"Optimizer should be SGD\"\n",
    "            except KeyError:\n",
    "                print(\n",
    "                    \"Optimizer Initialization Error: 'optimizer' key missing from helper return dictionary\"\n",
    "                )\n",
    "                sys.exit(1)\n",
    "            except AssertionError as e:\n",
    "                print(e)\n",
    "                sys.exit(1)\n",
    "\n",
    "            try:\n",
    "                self.criterion = self.init[\"criterion\"]\n",
    "                assert (\n",
    "                    self.criterion.__class__ == LossFunction\n",
    "                ), \"Criterion should be a PyTorch loss function\"\n",
    "            except KeyError:\n",
    "                print(\n",
    "                    \"Criterion Initialization Error: 'criterion' key missing from helper return dictionary\"\n",
    "                )\n",
    "                sys.exit(1)\n",
    "            except AssertionError as e:\n",
    "                print(e)\n",
    "                sys.exit(1)\n",
    "\n",
    "            if self.lr_scheduler:\n",
    "                self.scheduler = StepLR(\n",
    "                    optimizer=self.optimizer, step_size=self.step_size, gamma=self.gamma\n",
    "                )\n",
    "\n",
    "            self.model = self.model.to(self.device)\n",
    "            self.criterion = self.criterion.to(self.device)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected Error in Trainer Initialization: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        self.train_models = config_files()[\"artifacts\"][\"train_models\"]\n",
    "        self.best_model = config_files()[\"artifacts\"][\"best_model\"]\n",
    "        self.metrics_path = config_files()[\"artifacts\"][\"metrics\"]\n",
    "\n",
    "        self.model_history = {\n",
    "            \"train_loss\": [],\n",
    "            \"test_loss\": [],\n",
    "            \"train_accuracy\": [],\n",
    "            \"test_accuracy\": [],\n",
    "        }\n",
    "\n",
    "        self.loss = float(\"inf\")\n",
    "\n",
    "    def l1_regularizer(self, model):\n",
    "        if isinstance(model, MultiModalClassifier):\n",
    "            return self.l1_lambda * sum(\n",
    "                torch.norm(input=params, p=1) for params in model.parameters()\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Model must be an instance of MultiModalClassifier\")\n",
    "\n",
    "    def l2_regularizer(self):\n",
    "        if isinstance(model, MultiModalClassifier):\n",
    "            return self.l2_lambda * sum(\n",
    "                torch.norm(input=params, p=2) for params in model.parameters()\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Model must be an instance of MultiModalClassifier\")\n",
    "\n",
    "    def saved_checkpoints(self, train_loss: float, epoch: int):\n",
    "        if self.loss > train_loss:\n",
    "            self.loss = train_loss\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model\": self.model.state_dict(),\n",
    "                    \"optimizer\": self.optimizer.state_dict(),\n",
    "                    \"train_loss\": self.loss,\n",
    "                    \"epoch\": epoch,\n",
    "                },\n",
    "                os.path.join(self.best_model, \"best_model.pth\"),\n",
    "            )\n",
    "        torch.save(\n",
    "            self.model.state_dict(),\n",
    "            os.path.join(self.train_models, \"model{}.pth\".format(epoch)),\n",
    "        )\n",
    "\n",
    "    def update_train(self, **kwargs):\n",
    "        predicted = kwargs[\"predicted\"].float()\n",
    "        labels = kwargs[\"labels\"].float()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        loss = self.criterion(predicted, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def display_progress(\n",
    "        self,\n",
    "        train_loss: list,\n",
    "        valid_loss: list,\n",
    "        train_accuracy: list,\n",
    "        valid_accuracy: list,\n",
    "        kwargs: dict,\n",
    "    ):\n",
    "        if (\n",
    "            isinstance(train_loss, list)\n",
    "            and isinstance(valid_loss, list)\n",
    "            and isinstance(train_accuracy, list)\n",
    "            and isinstance(valid_accuracy, list)\n",
    "        ):\n",
    "            train_loss = np.mean(train_loss)\n",
    "            valid_loss = np.mean(valid_loss)\n",
    "            train_accuracy = np.mean(train_accuracy)\n",
    "            valid_accuracy = np.mean(valid_accuracy)\n",
    "            number_of_epochs = self.epochs\n",
    "            epoch = kwargs[\"epochs\"]\n",
    "\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{number_of_epochs}] | \"\n",
    "                f\"Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Test Loss: {valid_loss:.4f} | \"\n",
    "                f\"Train Acc: {train_accuracy:.4f} | \"\n",
    "                f\"Valid Acc: {valid_accuracy:.4f}\"\n",
    "            )\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in tqdm(range(self.epochs), desc=\"Training\"):\n",
    "            train_loss = []\n",
    "            valid_loss = []\n",
    "            train_accuracy = []\n",
    "            valid_accuracy = []\n",
    "\n",
    "            for idx, (images, texts, labels) in enumerate(self.train_dataloader):\n",
    "                images = images.to(self.device)\n",
    "                texts = texts.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                predicted = self.model(image=images, text=texts)\n",
    "\n",
    "                train_loss.append(self.update_train(predicted=predicted, labels=labels))\n",
    "\n",
    "                predicted = torch.where(predicted > 0.5, 1, 0)\n",
    "                predicted = predicted.detach().cpu().numpy()\n",
    "                labels = labels.detach().cpu().numpy()\n",
    "\n",
    "                train_accuracy.append(accuracy_score(predicted, labels))\n",
    "\n",
    "            for idx, (images, texts, labels) in enumerate(self.test_dataloader):\n",
    "                images = images.to(self.device)\n",
    "                texts = texts.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                predicted = self.model(image=images, text=texts)\n",
    "\n",
    "                valid_loss.append(\n",
    "                    self.criterion(predicted.float(), labels.float()).item()\n",
    "                )\n",
    "\n",
    "                predicted = torch.where(predicted > 0.5, 1, 0)\n",
    "                predicted = predicted.detach().cpu().numpy()\n",
    "                labels = labels.detach().cpu().numpy()\n",
    "\n",
    "                valid_accuracy.append(accuracy_score(predicted, labels))\n",
    "\n",
    "            if self.lr_scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "            try:\n",
    "                self.display_progress(\n",
    "                    train_loss=train_loss,\n",
    "                    valid_loss=valid_loss,\n",
    "                    train_accuracy=train_accuracy,\n",
    "                    valid_accuracy=valid_accuracy,\n",
    "                    kwargs={\"epochs\": epoch + 1},\n",
    "                )\n",
    "\n",
    "                train_loss_mean = np.mean(train_loss)\n",
    "                valid_loss_mean = np.mean(valid_loss)\n",
    "                train_acc_mean = np.mean(train_accuracy)\n",
    "                valid_acc_mean = np.mean(valid_accuracy)\n",
    "\n",
    "                self.saved_checkpoints(train_loss=train_loss_mean, epoch=epoch + 1)\n",
    "\n",
    "                self.model_history[\"train_loss\"].append(train_loss_mean)\n",
    "                self.model_history[\"train_accuracy\"].append(train_acc_mean)\n",
    "                self.model_history[\"test_loss\"].append(valid_loss_mean)\n",
    "                self.model_history[\"test_accuracy\"].append(valid_acc_mean)\n",
    "\n",
    "                plot_images(predicted=True, device=self.device, model = self.model, epoch=epoch+1)\n",
    "\n",
    "            except KeyError as e:\n",
    "                print(f\"[Error] Missing key in function arguments: {e}\")\n",
    "            except TypeError as e:\n",
    "                print(f\"[Error] Type mismatch in function arguments: {e}\")\n",
    "            except ValueError as e:\n",
    "                print(f\"[Error] Invalid value encountered: {e}\")\n",
    "            except FileNotFoundError:\n",
    "                print(\n",
    "                    \"Error: Checkpoint directory not found. Ensure the save path exists.\"\n",
    "                )\n",
    "            except PermissionError:\n",
    "                print(\n",
    "                    \"Error: Permission denied. Cannot write to the checkpoint directory.\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"[Unexpected Error] {e}\")\n",
    "\n",
    "        dump_file(\n",
    "            value=self.model_history,\n",
    "            filename=os.path.join(self.metrics_path, \"history.pkl\"),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def display_history():\n",
    "        metrics_path = config_files()[\"artifacts\"][\"metrics\"]\n",
    "        history = load_file(filename=os.path.join(metrics_path, \"history.pkl\"))\n",
    "        if history is not None:\n",
    "            _, axes = plt.subplots(2, 2, figsize=(10, 10), sharex=True)\n",
    "\n",
    "            axes[0, 0].plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "            axes[0, 0].plot(history[\"test_loss\"], label=\"Test Loss\")\n",
    "            axes[0, 0].set_title(\"Loss\")\n",
    "            axes[0, 0].set_xlabel(\"Epochs\")\n",
    "            axes[0, 0].legend()\n",
    "\n",
    "            axes[0, 1].plot(history[\"train_accuracy\"], label=\"Train Accuracy\")\n",
    "            axes[0, 1].plot(history[\"test_accuracy\"], label=\"Test Accuracy\")\n",
    "            axes[0, 1].set_title(\"Accuracy\")\n",
    "            axes[0, 1].set_xlabel(\"Epochs\")\n",
    "            axes[0, 1].legend()\n",
    "\n",
    "            axes[1, 0].axis(\"off\")\n",
    "            axes[1, 1].axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(metrics_path, \"history.png\"))\n",
    "            plt.show()\n",
    "            print(\"History saved as 'history.png' in the metrics folder\".capitalize())\n",
    "        else:\n",
    "            print(\"No history found\".capitalize())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = config_files()[\"trainer\"][\"model\"]\n",
    "    epochs = config_files()[\"trainer\"][\"epochs\"]\n",
    "    lr = float(config_files()[\"trainer\"][\"lr\"])\n",
    "    beta1 = float(config_files()[\"trainer\"][\"beta1\"])\n",
    "    beta2 = float(config_files()[\"trainer\"][\"beta2\"])\n",
    "    momentum = config_files()[\"trainer\"][\"momentum\"]\n",
    "    step_size = config_files()[\"trainer\"][\"step_size\"]\n",
    "    gamma = config_files()[\"trainer\"][\"gamma\"]\n",
    "    l1_lambda = config_files()[\"trainer\"][\"l1_lambda\"]\n",
    "    l2_lambda = config_files()[\"trainer\"][\"l2_lambda\"]\n",
    "    device = config_files()[\"trainer\"][\"device\"]\n",
    "    adam = config_files()[\"trainer\"][\"adam\"]\n",
    "    SGD = config_files()[\"trainer\"][\"SGD\"]\n",
    "    l1_regularization = config_files()[\"trainer\"][\"l1_regularization\"]\n",
    "    l2_regularization = config_files()[\"trainer\"][\"l2_regularization\"]\n",
    "    lr_scheduler = config_files()[\"trainer\"][\"lr_scheduler\"]\n",
    "    verbose = config_files()[\"trainer\"][\"verbose\"]\n",
    "    mlflow = config_files()[\"trainer\"][\"mlflow\"]\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=None,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta1=beta1,\n",
    "        beta2=beta2,\n",
    "        momentum=momentum,\n",
    "        step_size=step_size,\n",
    "        gamma=gamma,\n",
    "        l1_lambda=l1_lambda,\n",
    "        l2_lambda=l2_lambda,\n",
    "        device=device,\n",
    "        adam=adam,\n",
    "        SGD=SGD,\n",
    "        l1_regularization=l1_regularization,\n",
    "        l2_regularization=l2_regularization,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        verbose=verbose,\n",
    "        mlflow=mlflow,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPSG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
